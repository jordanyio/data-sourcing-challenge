{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries and Set Up Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import requests\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables from the .env in the local environment\n",
    "load_dotenv()\n",
    "\n",
    "nyt_api_key = os.getenv(\"NYT_API_KEY\")\n",
    "tmdb_api_key = os.getenv(\"TMDB_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the New York Times API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base URL\n",
    "url = \"https://api.nytimes.com/svc/search/v2/articlesearch.json?\"\n",
    "\n",
    "# Filter for movie reviews with \"love\" in the headline\n",
    "# section_name should be \"Movies\"\n",
    "# type_of_material should be \"Review\"\n",
    "filter_query = 'section_name:\"Movies\" AND type_of_material:\"Review\" AND headline:\"love\"'\n",
    "\n",
    "# Use a sort filter, sort by newest\n",
    "sort = \"newest\"\n",
    "\n",
    "# Select the following fields to return:\n",
    "# headline, web_url, snippet, source, keywords, pub_date, byline, word_count\n",
    "field_list = \"headline,web_url,snippet,source,keywords,pub_date,byline,word_count\"\n",
    "\n",
    "# Search for reviews published between a begin and end date\n",
    "begin_date = \"20130101\"\n",
    "end_date = \"20230531\"\n",
    "\n",
    "# Build URL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on page 0, skipping.\n",
      "Error on page 1, skipping.\n",
      "Error on page 2, skipping.\n",
      "Error on page 3, skipping.\n",
      "Error on page 4, skipping.\n",
      "Error on page 5, skipping.\n",
      "Error on page 6, skipping.\n",
      "Error on page 7, skipping.\n",
      "Error on page 8, skipping.\n",
      "Error on page 9, skipping.\n",
      "Error on page 10, skipping.\n",
      "Error on page 11, skipping.\n",
      "Error on page 12, skipping.\n",
      "Error on page 13, skipping.\n",
      "Error on page 14, skipping.\n",
      "Error on page 15, skipping.\n",
      "Error on page 16, skipping.\n",
      "Error on page 17, skipping.\n",
      "Error on page 18, skipping.\n",
      "Error on page 19, skipping.\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store the reviews\n",
    "reviews_list = []\n",
    "\n",
    "# loop through pages 0-19\n",
    "for page in range(20):\n",
    "    # create query with a page number\n",
    "    query = {\n",
    "        'api-key': nyt_api_key,\n",
    "        'q': filter_query,\n",
    "        'sort': sort,\n",
    "        'fl': field_list,\n",
    "        'begin_date': begin_date,\n",
    "        'end_date': end_date,\n",
    "        'page': page\n",
    "    }\n",
    "\n",
    "    # Make a \"GET\" request and retrieve the JSON\n",
    "    response = requests.get(url, params=query)\n",
    "    reviews = response.json()\n",
    "\n",
    "    # Add a twelve second interval between queries to stay within API query limits\n",
    "    time.sleep(12)\n",
    "\n",
    "    # Try and save the reviews to the reviews_list\n",
    "    try:\n",
    "        if reviews[\"response\"][\"docs\"]:\n",
    "            # loop through the reviews[\"response\"][\"docs\"] and append each review to the list\n",
    "            for review in reviews[\"response\"][\"docs\"]:\n",
    "                reviews_list.append(review)\n",
    "            # Print the page that was just retrieved\n",
    "            print(f\"Retrieved page {page}\")\n",
    "        else:\n",
    "            # Print the page number that had no results then break from the loop\n",
    "            print(f\"No results on page {page}, stopping.\")\n",
    "            break\n",
    "    except KeyError:\n",
    "        print(f\"Error on page {page}, skipping.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Preview the first 5 results in JSON format\n",
    "# Use json.dumps with argument indent=4 to format data\n",
    "print(json.dumps(reviews_list[:5], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert reviews_list to a Pandas DataFrame using json_normalize()\n",
    "df_reviews = pd.json_normalize(reviews_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'headline.main'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Extract the title from the \"headline.main\" column and\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# save it to a new column \"title\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Title is between unicode characters \\u2018 and \\u2019. \u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# End string should include \" Review\" to avoid cutting title early\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m reviews_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mreviews_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheadline.main\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mextract(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mu2018(.*?) Review\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mu2019\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\dev-ai\\lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\dev-ai\\lib\\site-packages\\pandas\\core\\indexes\\range.py:349\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m--> 349\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'headline.main'"
     ]
    }
   ],
   "source": [
    "# Extract the title from the \"headline.main\" column and\n",
    "# save it to a new column \"title\"\n",
    "# Title is between unicode characters \\u2018 and \\u2019. \n",
    "# End string should include \" Review\" to avoid cutting title early\n",
    "reviews_df['title'] = reviews_df['headline.main'].str.extract('\\u2018(.*?)\\u2019 Review')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 'name' and 'value' from items in \"keywords\" column\n",
    "def extract_keywords(keyword_list):\n",
    "    extracted_keywords = \"\"\n",
    "    for item in keyword_list:\n",
    "        # Extract 'name' and 'value'\n",
    "        keyword = f\"{item['name']}: {item['value']};\" \n",
    "        # Append the keyword item to the extracted_keywords list\n",
    "        extracted_keywords += keyword\n",
    "    return extracted_keywords\n",
    "\n",
    "# Fix the \"keywords\" column by converting cells from a list to a string\n",
    "df_reviews['keywords'] = df_reviews['keywords'].apply(extract_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a list from the \"title\" column using to_list()\n",
    "# These titles will be used in the query for The Movie Database\n",
    "titles_list = reviews_df['title'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access The Movie Database API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare The Movie Database query\n",
    "url = \"https://api.themoviedb.org/3/search/movie?query=\"\n",
    "tmdb_key_string = \"&api_key=\" + tmdb_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the results\n",
    "tmdb_movies_list = []\n",
    "\n",
    "# Create a request counter to sleep the requests after a multiple\n",
    "# of 50 requests\n",
    "request_counter = 0\n",
    "\n",
    "# Loop through the titles\n",
    "for title in title_list:\n",
    "    # Check if we need to sleep before making a request\n",
    "    if request_counter % 50 == 0 and request_counter != 0:\n",
    "        time.sleep(60)\n",
    "\n",
    "    # Add 1 to the request counter\n",
    "    request_counter += 1\n",
    "\n",
    "    # Perform a \"GET\" request for The Movie Database\n",
    "    search_url = url + title + tmdb_key_string\n",
    "    response = requests.get(search_url)\n",
    "\n",
    "    # Include a try clause to search for the full movie details.\n",
    "    # Use the except clause to print out a statement if a movie\n",
    "    # is not found.\n",
    "    try:\n",
    "        movie_id = response.json()['results'][0]['id']\n",
    "\n",
    "        # Get movie id\n",
    "        movie_url = f\"https://api.themoviedb.org/3/movie/{movie_id}?api_key={tmdb_api_key}\"\n",
    "\n",
    "        # Make a request for the full movie details\n",
    "        movie_response = requests.get(movie_url)\n",
    "        movie_details = movie_response.json()\n",
    "\n",
    "        # Execute \"GET\" request with url\n",
    "        genres = [genre['name'] for genre in movie_details['genres']]\n",
    "        languages = [lang['english_name'] for lang in movie_details['spoken_languages']]\n",
    "        countries = [country['name'] for country in movie_details['production_countries']]\n",
    "\n",
    "        # Extract the genre names into a list\n",
    "        # Extract the spoken_languages' English name into a list\n",
    "        # Extract the production_countries' name into a list\n",
    "\n",
    "        # Add the relevant data to a dictionary and\n",
    "        # append it to the tmdb_movies_list list\n",
    "        tmdb_movies_list.append({\n",
    "            'title': title,\n",
    "            'genres': genres,\n",
    "            'languages': languages,\n",
    "            'countries': countries\n",
    "        })\n",
    "\n",
    "        # Print out the title that was found\n",
    "        print(f\"Found: {title}\")\n",
    "\n",
    "    except IndexError:\n",
    "        print(f\"No TMDB entry found for: {title}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first 5 results in JSON format\n",
    "# Use json.dumps with argument indent=4 to format data\n",
    "print(json.dumps(tmdb_movies_list[:5], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results to a DataFrame\n",
    "tmdb_df = pd.DataFrame(tmdb_movies_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge and Clean the Data for Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge the New York Times reviews and TMDB DataFrames on title\n",
    "merged_df = pd.merge(df_reviews, df_tmdb, on='title')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove list brackets and quotation marks on the columns containing lists\n",
    "# Create a list of the columns that need fixing\n",
    "list_columns = ['genres', 'languages', 'countries']\n",
    "\n",
    "# Create a list of characters to remove\n",
    "chars_to_remove = ['[', ']', \"'\"]\n",
    "\n",
    "# Loop through the list of columns to fix\n",
    "for col in list_columns:\n",
    "    # Convert the column to type 'str'\n",
    "    merged_df[col] = merged_df[col].astype(str)\n",
    "\n",
    "    # Loop through characters to remove\n",
    "    for char in chars_to_remove:\n",
    "        merged_df[col] = merged_df[col].str.replace(char, '')\n",
    "\n",
    "# Display the fixed DataFrame\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop \"byline.person\" column\n",
    "merged_df.drop(columns=['byline.person'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete duplicate rows and reset index\n",
    "merged_df.drop_duplicates(inplace=True)\n",
    "merged_df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to CSV without the index\n",
    "merged_df.to_csv('movie_reviews_data.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
